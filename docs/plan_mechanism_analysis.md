# Plan Mechanism: Design Rationale

## Why a Structured Plan?

The evaluation framework introduces a `<plan>` step where the therapist model declares 1-2 strategies from the IRT strategy taxonomy (now 7 categories; see `strategy_taxonomy_evolution.md`) before responding. The original motivation was twofold: improve response quality through structured reasoning (similar to Chain-of-Thought), and create a transparent, auditable trace of the model's intended therapeutic approach.

However, framing this as "explainability" raises concerns. Recent work on CoT faithfulness (Turpin et al., 2023; Lanham et al., 2023) has shown that language model explanations do not necessarily reflect internal reasoning — they can be post-hoc rationalizations that look plausible but are not causally connected to the model's actual decision process. When the model outputs `<plan>empowerment / mastery</plan>`, there is no guarantee this represents genuine therapeutic intent rather than token-level pattern completion.

This does not make the plan useless. It changes what we can credibly claim about it.

## What the Plan Does (and Doesn't) Provide

The plan is best understood as a **declared intent mechanism** rather than an explanation. It does not answer "why did the model choose this strategy?" but it does enable:

- **Structured measurement** — Jaccard similarity over plan sets (Level 3.1) quantifies whether the model makes stable therapeutic decisions across stochastic trials, independent of how it phrases the response.
- **Alignment verification** — Level 3.3 can check whether the model's actual response matches its declared strategy, testing instruction adherence rather than self-awareness.
- **Clinical oversight** — In a deployment scenario, the plan provides a reviewable summary of the model's intended approach before the full response is delivered to a patient.
- **Taxonomic constraint** — Forcing strategy selection through a fixed taxonomy keeps the model within the IRT framework, reducing the risk of off-protocol therapeutic interventions.

None of these claims require the plan to faithfully represent internal reasoning. They hold regardless of whether the plan is "genuine" or a surface-level label.

## Fused vs. Chained Generation

Two generation modes were implemented and compared:

In **fused mode**, the model produces both plan and response in a single output (CoT-style). The plan genuinely conditions the response tokens that follow. However, the combined task introduces more variance — the model is less consistent in strategy selection when simultaneously composing a therapeutic response. Empirically, Jaccard dropped as low as 0.47 on complex vignettes (resistant).

In **chained mode**, the plan is generated by a separate call, then injected into the response prompt as an explicit instruction ("Your therapeutic plan for this response: ..."). This cleanly separates strategy selection from response generation, and avoids the CoT faithfulness debate entirely — we are not claiming the plan explains reasoning, only testing whether the model can (a) consistently select strategies and (b) follow them when instructed.

| Vignette | Fused Jaccard | Chained Jaccard | Fused BERTScore F1 | Chained BERTScore F1 |
|----------|:---:|:---:|:---:|:---:|
| anxious | 0.600 | 0.733 | 0.730 | 0.757 |
| avoidant | 1.000 | 1.000 | 0.736 | 0.770 |
| cooperative | 1.000 | 1.000 | 0.690 | 0.794 |
| resistant | 0.467 | 1.000 | 0.697 | 0.799 |
| skeptic | 0.733 | 1.000 | 0.758 | 0.834 |
| trauma | 0.533 | 1.000 | 0.757 | 0.700 |

*Llama 70B, t=0.5, n=5 per vignette.*

Chained mode produces more stable plans and generally more consistent responses. It is also the more defensible experimental setup because each level of the evaluation framework measures a distinct, well-scoped property:

- Level 3.1 → Does the model select the same strategies? (plan consistency)
- Level 3.2 → Does the model say the same thing? (response consistency)
- Level 3.3 → Does the model do what it said it would? (plan-output alignment)

## References

- Turpin, M., Michael, J., Perez, E., & Bowman, S. (2023). Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. *NeurIPS 2023*. https://arxiv.org/abs/2305.04388
- Lanham, T., et al. (2023). Measuring Faithfulness in Chain-of-Thought Reasoning. *arXiv:2307.13702*. https://arxiv.org/abs/2307.13702
- Wei, J., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. *NeurIPS 2022*. https://arxiv.org/abs/2201.11903
